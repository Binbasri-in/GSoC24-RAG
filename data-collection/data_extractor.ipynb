{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Extractor with other tools\n",
    "- I tried both LlamaIndex but it was not satisfied, so here I tried different ways."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Extractor with Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function calling for GSoC 2024 ideas extraction\n",
    "import google.generativeai as genai\n",
    "import textwrap\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "\n",
    "\n",
    "genai.configure(api_key=\"\")\n",
    "\n",
    "# Set up the model\n",
    "generation_config = {\n",
    "  \"temperature\": 0.9,\n",
    "  \"top_p\": 1,\n",
    "  \"top_k\": 1,\n",
    "  \"max_output_tokens\": 2048,\n",
    "}\n",
    "\n",
    "safety_settings = [\n",
    "  {\n",
    "    \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
    "    \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"\n",
    "  },\n",
    "  {\n",
    "    \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
    "    \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"\n",
    "  },\n",
    "  {\n",
    "    \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
    "    \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"\n",
    "  },\n",
    "  {\n",
    "    \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
    "    \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"\n",
    "  },\n",
    "]\n",
    "\n",
    "model = genai.GenerativeModel(model_name=\"gemini-1.0-pro\",\n",
    "                              generation_config=generation_config,\n",
    "                              safety_settings=safety_settings)\n",
    "\n",
    "gsoc_df = pd.read_csv(\"gsoc_organizations_ideas_link.csv\")\n",
    "files = os.listdir('results2')\n",
    "\n",
    "# for each file name split it using the underscore and the first part is the main name, save them in a list\n",
    "file_names = [file.split('_')[0] for file in files]\n",
    "print(file_names)\n",
    "\n",
    "# For each text file\n",
    "for index, row in gsoc_df.iterrows():\n",
    "    if row['ideas_link_file'] == \"Not Found\" or row['name'] in file_names:\n",
    "        continue\n",
    "    \n",
    "    with open(f\"./ideas_link_data/{row['ideas_link_file']}\", 'r') as file:\n",
    "        content = file.read()\n",
    "              \n",
    "    response = model.generate_content(textwrap.dedent(\"\"\"\\\n",
    "          Please return JSON descriping the the ideas from the given content using the following schema:\n",
    "\n",
    "          {\"ideas\": list[IDEA]}\n",
    "\n",
    "          IDEA = {\"title\": str, \"description\": str, \"skills\": str, \"difficulty\": str, \"duration\": str,  \"related_url\": list[str]}\n",
    "\n",
    "          All fields are required. the related_url field, whitch is the related URLs.\n",
    "\n",
    "          Important: Only return a single piece of valid JSON text.\n",
    "\n",
    "          Here is the content:\n",
    "\n",
    "          \"\"\") + content)\n",
    "\n",
    "    json_text = response.text.strip('`\\r\\n ').removeprefix('json')\n",
    "    data = json.dumps(json.loads(json_text), indent=4)\n",
    "      \n",
    "    with open(f\"results2/{row['name']}_ideas.json\", 'w') as file:\n",
    "          file.write(data)\n",
    "    print(f\"Done with {row['name']}\")\n",
    "print(\"All done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.ai.generativelanguage as glm\n",
    "import google.generativeai as genai\n",
    "\n",
    "gsoc_idea = glm.Schema(\n",
    "    type = glm.Type.OBJECT,\n",
    "    properties = {\n",
    "        'title':  glm.Schema(type=glm.Type.STRING),\n",
    "        'description':  glm.Schema(type=glm.Type.STRING),\n",
    "        'skills': glm.Schema(type=glm.Type.ARRAY, items=glm.Schema(type=glm.Type.STRING)),\n",
    "        'duration': glm.Schema(type=glm.Type.STRING),\n",
    "        'difficulty': glm.Schema(type=glm.Type.STRING),\n",
    "        'related_url': glm.Schema(type=glm.Type.ARRAY, items=glm.Schema(type=glm.Type.STRING))\n",
    "    },\n",
    "    required=['title', 'description', 'skills', 'duration', 'difficulty']\n",
    ")\n",
    "\n",
    "gsoc_ideas = glm.Schema(\n",
    "    type=glm.Type.ARRAY,\n",
    "    items=gsoc_idea\n",
    ")\n",
    "\n",
    "add_to_database = glm.FunctionDeclaration(\n",
    "    name=\"add_to_database\",\n",
    "    description=textwrap.dedent(\"\"\"\\\n",
    "        Adds entities to the database.\n",
    "        \"\"\"),\n",
    "    parameters=glm.Schema(\n",
    "        type=glm.Type.OBJECT,\n",
    "        properties = {\n",
    "            'gsoc_ideas': gsoc_ideas\n",
    "        }\n",
    "    )\n",
    ")\n",
    "\n",
    "model = genai.GenerativeModel(\n",
    "    model_name='gemini-1.0-pro',\n",
    "    tools = [add_to_database])\n",
    "\n",
    "gsoc_df = pd.read_csv(\"gsoc_organizations_ideas_link.csv\")\n",
    "files = os.listdir('results2')\n",
    "\n",
    "# for each file name split it using the underscore and the first part is the main name, save them in a list\n",
    "file_names = [file.split('_')[0] for file in files]\n",
    "print(file_names)\n",
    "\n",
    "# For each text file\n",
    "for index, row in gsoc_df.iterrows():\n",
    "    if row['ideas_link_file'] == \"Not Found\" or row['name'] in file_names:\n",
    "        continue\n",
    "    \n",
    "    with open(f\"./ideas_link_data/{row['ideas_link_file']}\", 'r') as file:\n",
    "        content = file.read()\n",
    "        \n",
    "    result = model.generate_content(f\"\"\"\n",
    "    Please add the ideas from this content to the database:\n",
    "\n",
    "    {content}\n",
    "    \"\"\")\n",
    "\n",
    "    if 'function_call' in result.candidates[0].content.parts[0]:\n",
    "        fc = result.candidates[0].content.parts[0].function_call\n",
    "        data = json.dumps(type(fc).to_dict(fc), indent=4)\n",
    "        with open(f\"results2/{row['name']}_add_to_database.json\", 'w') as file:\n",
    "            file.write(data)\n",
    "    print(f\"Done with {row['name']}\")\n",
    "print(\"All done\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Extractor with Cloudflare AI worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from IPython.display import Markdown, display\n",
    "import json\n",
    "\n",
    "model = \"@hf/thebloke/deepseek-coder-6.7b-instruct-awq\"\n",
    "\n",
    "# JSON schema\n",
    "json_schema = \"\"\"\n",
    "{\n",
    "    \"title\": \"GSoC Project Idea\",\n",
    "    \"description\": \"An idea from an organization from Google Summer of Code\",\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"organizationName\": {\n",
    "            \"description\": \"The name of the organization proposing the project\",\n",
    "            \"type\": \"string\"\n",
    "        },\n",
    "        \"projectTitle\": {\n",
    "            \"description\": \"The title of the project\",\n",
    "            \"type\": \"string\"\n",
    "        },\n",
    "        \"projectDescription\": {\n",
    "            \"description\": \"A brief description of the project\",\n",
    "            \"type\": \"string\"\n",
    "        },\n",
    "        \"skills\": {\n",
    "            \"description\": \"Skills or tech stack required for the project\",\n",
    "            \"type\": \"array\",\n",
    "            \"items\": {\n",
    "                \"type\": \"string\"\n",
    "            }\n",
    "        },\n",
    "        \"difficulty\": {\n",
    "            \"description\": \"The difficulty level of the project\",\n",
    "            \"type\": \"string\"\n",
    "        },\n",
    "        \"duration\": {\n",
    "            \"description\": \"The estimated duration of the project in hours\",\n",
    "            \"type\": \"integer\"\n",
    "        },\n",
    "        \"relatedLinks\": {\n",
    "            \"description\": \"Related links for the project\",\n",
    "            \"type\": \"array\",\n",
    "            \"items\": {\n",
    "                \"type\": \"string\"\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"required\": [\"organizationName\", \"projectTitle\", \"projectDescription\", \"skills\", \"difficulty\"]\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "system_prompt = f\"\"\"\n",
    "The text below describes a project idea for Google Summer of Code. Create a JSON object from the description to match the JSON schema provided. The description may have more than one idea and you should give me all the available objects.\n",
    "\n",
    "<BEGIN JSON SCHEMA>\n",
    "{json_schema}\n",
    "<END JSON SCHEMA>\n",
    "\n",
    "Return JSON only. Do not explain or provide usage examples. just give me the JSON object that you can return and for parts you don't know just leave them empty.\n",
    "\"\"\"\n",
    "\n",
    "gsoc_df = pd.read_csv(\"gsoc_organizations_ideas_link.csv\")\n",
    "account_id = \"4da66dac8f0c0483794586300c5ccc66\"\n",
    "api_token = \"kOVr_YX01r4cveU4iPRy1Toe9rDODTedw0YeofF1\"\n",
    "\n",
    "# delete the files that already processed\n",
    "import os\n",
    "# load all the file names in the directory\n",
    "files = os.listdir('results2')\n",
    "\n",
    "# for each file name split it using the underscore and the first part is the main name, save them in a list\n",
    "file_names = [file.split('_')[0] for file in files]\n",
    "print(file_names)\n",
    "\n",
    "# Initialize an empty DataFrame to store the parsed JSON responses\n",
    "parsed_df = pd.DataFrame()\n",
    "\n",
    "for index, row in gsoc_df.iterrows():\n",
    "    if row['ideas_link_file'] in file_names:\n",
    "        continue\n",
    "    \n",
    "    with open(f\"./ideas_link_data/{row['ideas_link_file']}\", 'r') as file:\n",
    "        prompt = file.read()\n",
    "            \n",
    "    print(f\"Processing idea {index + 1}...\")\n",
    "    \n",
    "    # Split the prompt into chunks of 6144 characters\n",
    "    chunks = [prompt[i:i+2000] for i in range(0, len(prompt), 2000)]\n",
    "    \n",
    "    for chunk in chunks:\n",
    "        response = requests.post(\n",
    "                f\"https://api.cloudflare.com/client/v4/accounts/{account_id}/ai/run/@cf/meta/llama-2-7b-chat-fp16\",\n",
    "                headers={\"Authorization\": f\"Bearer {api_token}\"},\n",
    "                json={\"messages\": [\n",
    "                        {\"role\": \"system\", \"content\": system_prompt},\n",
    "                        {\"role\": \"user\", \"content\": chunk}\n",
    "                ]}\n",
    "        )\n",
    "        \n",
    "        inference = response.json()\n",
    "        response = inference[\"result\"][\"response\"]\n",
    "\n",
    "        print(response.strip())\n",
    "        try:\n",
    "            # Parse the JSON response\n",
    "            user_info = json.loads(response.strip())\n",
    "\n",
    "            # Append the parsed JSON response to the DataFrame\n",
    "            parsed_df = parsed_df.append(user_info, ignore_index=True)\n",
    "            \n",
    "            print(f\"Successfully processed idea {index + 1}!\")\n",
    "        except:\n",
    "            print(f\"Failed to process idea {index + 1}!\")\n",
    "            continue\n",
    "        \n",
    "# Save the parsed DataFrame to a CSV file\n",
    "parsed_df.to_csv('parsed_data.csv', index=False)\n",
    "\n",
    "# Display the parsed DataFrame\n",
    "display(parsed_df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
